{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a24a899",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries and Initialize SparkSession\n",
    "Import `SparkSession` and create your Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15192926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9d5b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/26 08:48:48 WARN Utils: Your hostname, codespaces-1164d4, resolves to a loopback address: 127.0.0.1; using 10.0.11.153 instead (on interface eth0)\n",
      "25/07/26 08:48:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/26 08:48:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Chapter 5 - Basic Structured Operations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3714e2",
   "metadata": {},
   "source": [
    "# 2. Basic DataFrame Creation and Schema\n",
    "Load JSON data and examine its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86873435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31dcca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the inferred schema\n",
    "spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7703e",
   "metadata": {},
   "source": [
    "# 3. Manual Schema Definition\n",
    "Define and apply a custom schema to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40784ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema) \\\n",
    "  .load(\"../data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1019e4",
   "metadata": {},
   "source": [
    "# 4. Working with Columns and Expressions\n",
    "Explore different ways to reference columns and create expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5067810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column references:\n",
      "Column<'someColumnName'>\n",
      "Column<'someColumnName'>\n",
      "\n",
      "Expression example:\n",
      "Column<'(((someCol + 5) * 200) - 6) < otherCol'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column, expr\n",
    "\n",
    "# Different ways to reference columns\n",
    "print(\"Column references:\")\n",
    "print(col(\"someColumnName\"))\n",
    "print(column(\"someColumnName\"))\n",
    "\n",
    "# Complex expressions\n",
    "print(\"\\nExpression example:\")\n",
    "print(expr(\"(((someCol + 5) * 200) - 6) < otherCol\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53b1da",
   "metadata": {},
   "source": [
    "# 5. Working with Rows\n",
    "Create and access Row objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc10dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: Hello\n",
      "Third element: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "print(f\"First element: {myRow[0]}\")\n",
    "print(f\"Third element: {myRow[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840d7c4",
   "metadata": {},
   "source": [
    "# 6. Create Temp View and Manual DataFrame\n",
    "Register DataFrame as SQL table and create DataFrame from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d3a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|NULL|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create temp view for SQL queries\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "# Create DataFrame from scratch\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"some\", StringType(), True),\n",
    "  StructField(\"col\", StringType(), True),\n",
    "  StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09c24b",
   "metadata": {},
   "source": [
    "# 7. Selecting Columns\n",
    "Various ways to select and manipulate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9d042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Simple column selection\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "# Multiple column selection\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c032b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Different ways to select the same column\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\")) \\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3484728",
   "metadata": {},
   "source": [
    "# 8. Column Aliasing and SelectExpr\n",
    "Rename columns and use SQL-like expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c094d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Column aliasing\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "\n",
    "# Nested aliasing\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")) \\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3160ea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# SelectExpr for SQL-like operations\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "# Add computed columns\n",
    "df.selectExpr(\n",
    "  \"*\", # all original columns\n",
    "  \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\") \\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f60753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregations with selectExpr\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16855b",
   "metadata": {},
   "source": [
    "# 9. Adding and Renaming Columns\n",
    "Use withColumn and withColumnRenamed to modify DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "906d0a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add literal column using select\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "\n",
    "# Add literal column using withColumn\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0f9f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "Columns after renaming:\n",
      "['dest', 'ORIGIN_COUNTRY_NAME', 'count']\n"
     ]
    }
   ],
   "source": [
    "# Add computed column\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")) \\\n",
    "  .show(2)\n",
    "\n",
    "# Rename column\n",
    "print(\"Columns after renaming:\")\n",
    "print(df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f76f5",
   "metadata": {},
   "source": [
    "# 10. Working with Special Column Names\n",
    "Handle columns with spaces or special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d681b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "Column names:\n",
      "['This Long Column-Name']\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with special column name\n",
    "dfWithLongColName = df.withColumn(\n",
    "    \"This Long Column-Name\",\n",
    "    expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "\n",
    "# Use backticks for special column names\n",
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\") \\\n",
    "  .show(2)\n",
    "\n",
    "print(\"Column names:\")\n",
    "print(dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a03cc7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'Column'. SQLSTATE: 42601 (line 1, pos 10)\n\n== SQL ==\nThis Long Column-Name\n----------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use backticks for special column names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdfWithLongColName\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThis Long Column-Name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m`This Long Column-Name` as `new col`\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[32m      5\u001b[39m   .show(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1005\u001b[39m, in \u001b[36mDataFrame.selectExpr\u001b[39m\u001b[34m(self, *expr)\u001b[39m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m   1004\u001b[39m     expr = expr[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mParseException\u001b[39m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'Column'. SQLSTATE: 42601 (line 1, pos 10)\n\n== SQL ==\nThis Long Column-Name\n----------^^^\n"
     ]
    }
   ],
   "source": [
    "# Use backticks for special column names\n",
    "dfWithLongColName.selectExpr(\n",
    "    \"This Long Column-Name\",\n",
    "    \"`This Long Column-Name` as `new col`\") \\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8a854",
   "metadata": {},
   "source": [
    "# 11. Filtering Data\n",
    "Use where/filter to subset your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef6dfa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Chain multiple where clauses\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\") \\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac682c",
   "metadata": {},
   "source": [
    "# 12. Getting Unique Records\n",
    "Count distinct values and get unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee79fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique origin-destination pairs: 256\n",
      "Unique origin countries: 125\n"
     ]
    }
   ],
   "source": [
    "# Count unique combinations\n",
    "unique_routes = df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n",
    "print(f\"Unique origin-destination pairs: {unique_routes}\")\n",
    "\n",
    "# Count unique origins\n",
    "unique_origins = df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
    "print(f\"Unique origin countries: {unique_origins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc88b30",
   "metadata": {},
   "source": [
    "# 13. Random Sampling and Splitting\n",
    "Sample and split DataFrames randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9945f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 138\n",
      "First split: 71, Second split: 185\n",
      "First smaller than second: True\n"
     ]
    }
   ],
   "source": [
    "# Random sampling\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "sample_count = df.sample(withReplacement, fraction, seed).count()\n",
    "print(f\"Sample size: {sample_count}\")\n",
    "\n",
    "# Random split\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "print(f\"First split: {dataFrames[0].count()}, Second split: {dataFrames[1].count()}\")\n",
    "print(f\"First smaller than second: {dataFrames[0].count() < dataFrames[1].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04cedf",
   "metadata": {},
   "source": [
    "# 14. Appending Rows (Union)\n",
    "Add new rows to existing DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c1c6f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new rows\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "  Row(\"New Country\", \"Other Country\", 5),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "# Union and filter\n",
    "df.union(newDF) \\\n",
    "  .where(\"count = 1\") \\\n",
    "  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16ff37",
   "metadata": {},
   "source": [
    "# 15. Sorting Data\n",
    "Sort and order DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac6e0768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort by count:\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Order by multiple columns:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Simple sorting\n",
    "print(\"Sort by count:\")\n",
    "df.sort(\"count\").show(5)\n",
    "\n",
    "print(\"\\nOrder by multiple columns:\")\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a0a3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descending order:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "Mixed ordering:\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "# Explicit ascending/descending\n",
    "print(\"Descending order:\")\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "\n",
    "print(\"\\nMixed ordering:\")\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0536b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /workspaces/Spark-The-Definitive-Guide/notebooks\n",
      "glob pattern: ../data/flight-data/json/*-summary.json\n",
      "Found 6 files:\n",
      "  ../data/flight-data/json/2014-summary.json\n",
      "  ../data/flight-data/json/2010-summary.json\n",
      "  ../data/flight-data/json/2015-summary.json\n",
      "  ../data/flight-data/json/2011-summary.json\n",
      "  ../data/flight-data/json/2012-summary.json\n",
      "  ../data/flight-data/json/2013-summary.json\n",
      "Sorted within partitions: 1502 rows\n"
     ]
    }
   ],
   "source": [
    "# 15. Sorting within partitions (for performance) – debug & load via explicit file list\n",
    "import os, glob\n",
    "\n",
    "pattern = \"../data/flight-data/json/*-summary.json\"\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"glob pattern:\", pattern)\n",
    "matches = glob.glob(pattern)\n",
    "print(f\"Found {len(matches)} files:\")\n",
    "for f in matches: print(\" \", f)\n",
    "\n",
    "if not matches:\n",
    "    print(\"⚠️  No files matched, falling back to single file\")\n",
    "    matches = [\"../data/flight-data/json/2010-summary.json\"]\n",
    "\n",
    "sorted_within_partitions = (\n",
    "    spark.read\n",
    "         .format(\"json\")\n",
    "         .load(matches)          # now loading the explicit list\n",
    "         .sortWithinPartitions(\"count\")\n",
    ")\n",
    "\n",
    "print(f\"Sorted within partitions: {sorted_within_partitions.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4e7b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted within partitions: 1502 rows\n"
     ]
    }
   ],
   "source": [
    "# Sort within partitions (for performance)\n",
    "import glob\n",
    "\n",
    "\n",
    "sorted_within_partitions = spark.read.format(\"json\").load(glob.glob(\"../data/flight-data/json/*-summary.json\")) \\\n",
    "  .sortWithinPartitions(\"count\")\n",
    "print(f\"Sorted within partitions: {sorted_within_partitions.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2fe06",
   "metadata": {},
   "source": [
    "# 16. Limiting Results\n",
    "Limit the number of rows returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64b4f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple limit\n",
    "df.limit(5).show()\n",
    "\n",
    "# Limit with ordering\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb254643",
   "metadata": {},
   "source": [
    "# 17. Repartitioning and Coalescing\n",
    "Control the physical layout of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "840ec44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current partitions: 1\n",
      "After repartition(5): 5\n",
      "After repartition by column: 1\n",
      "After repartition(5, column): 5\n",
      "After coalesce(2): 2\n",
      "After repartition by column: 1\n",
      "After repartition(5, column): 5\n",
      "After coalesce(2): 2\n"
     ]
    }
   ],
   "source": [
    "# Check current partitions\n",
    "print(f\"Current partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Different repartitioning strategies\n",
    "df_repart = df.repartition(5)\n",
    "print(f\"After repartition(5): {df_repart.rdd.getNumPartitions()}\")\n",
    "\n",
    "df_repart_col = df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "print(f\"After repartition by column: {df_repart_col.rdd.getNumPartitions()}\")\n",
    "\n",
    "df_repart_both = df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "print(f\"After repartition(5, column): {df_repart_both.rdd.getNumPartitions()}\")\n",
    "\n",
    "df_coalesced = df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)\n",
    "print(f\"After coalesce(2): {df_coalesced.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62981164",
   "metadata": {},
   "source": [
    "# 18. Collecting Data to Driver\n",
    "Bring data back to the driver for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using take(5):\n",
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344), Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]\n",
      "\n",
      "Using show():\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "\n",
      "Using show(5, False) - no truncation:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "\n",
      "Using show(5, False) - no truncation:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Using collect() - returns all rows as list:\n",
      "Collected 10 rows\n",
      "First row: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Using collect() - returns all rows as list:\n",
      "Collected 10 rows\n",
      "First row: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n"
     ]
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "\n",
    "# Different ways to inspect data\n",
    "print(\"Using take(5):\")\n",
    "print(collectDF.take(5))\n",
    "\n",
    "print(\"\\nUsing show():\")\n",
    "collectDF.show()\n",
    "\n",
    "print(\"\\nUsing show(5, False) - no truncation:\")\n",
    "collectDF.show(5, False)\n",
    "\n",
    "print(\"\\nUsing collect() - returns all rows as list:\")\n",
    "collected_data = collectDF.collect()\n",
    "print(f\"Collected {len(collected_data)} rows\")\n",
    "print(f\"First row: {collected_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c17a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using collect() - returns all rows as list:\n",
      "Collected 256 rows\n",
      "First row: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUsing collect() - returns all rows as list:\")\n",
    "collected_data = df.collect()\n",
    "print(f\"Collected {len(collected_data)} rows\")\n",
    "print(f\"First row: {collected_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7a545d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame có 100 dòng và 5 partitions.\n",
      "collectDF (limit 10) có 10 dòng.\n",
      "collectDF (limit 10) có 10 dòng.\n"
     ]
    }
   ],
   "source": [
    "# Tạo một DataFrame giả lập với số lượng hàng lớn và cố tình repartition để có nhiều partition hơn\n",
    "# (Để minh họa toLocalIterator tốt hơn, thường thì df sẽ có nhiều partition sẵn)\n",
    "data = []\n",
    "for i in range(1, 101): # 100 hàng dữ liệu\n",
    "    country_name = f\"Country_{i % 5}\" # Chia thành 5 loại quốc gia để có 5 partition sau repartition\n",
    "    data.append(Row(ID=i, Country=country_name, Value=i*10))\n",
    "\n",
    "df_large = spark.createDataFrame(data)\n",
    "\n",
    "# Repartition thành 5 partition để minh họa toLocalIterator duyệt qua từng partition\n",
    "# (Mặc dù df.limit(10) trong ví dụ sách không repartition,\n",
    "# nhưng để thấy rõ toLocalIterator hoạt động theo partition,\n",
    "# chúng ta sẽ tạo một df có nhiều partition ở đây)\n",
    "df_repartitioned = df_large.repartition(5, col(\"Country\"))\n",
    "\n",
    "print(f\"DataFrame có {df_repartitioned.count()} dòng và {df_repartitioned.rdd.getNumPartitions()} partitions.\")\n",
    "\n",
    "# Lấy 10 dòng đầu tiên của DataFrame này để làm ví dụ như trong sách\n",
    "# Lưu ý: limit() không thay đổi số lượng partition vật lý, nó chỉ giới hạn số dòng sau cùng\n",
    "collectDF = df_repartitioned.limit(10)\n",
    "print(f\"collectDF (limit 10) có {collectDF.count()} dòng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fbee761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using toLocalIterator():\n",
      "  Processing row: Row(ID=4, Country='Country_4', Value=40)\n",
      "  Processing row: Row(ID=5, Country='Country_0', Value=50)\n",
      "  Processing row: Row(ID=9, Country='Country_4', Value=90)\n",
      "  Processing row: Row(ID=10, Country='Country_0', Value=100)\n",
      "  Processing row: Row(ID=14, Country='Country_4', Value=140)\n",
      "  Processing row: Row(ID=15, Country='Country_0', Value=150)\n",
      "  Processing row: Row(ID=19, Country='Country_4', Value=190)\n",
      "  Processing row: Row(ID=20, Country='Country_0', Value=200)\n",
      "  Processing row: Row(ID=24, Country='Country_4', Value=240)\n",
      "  Processing row: Row(ID=25, Country='Country_0', Value=250)\n",
      "\n",
      "Successfully iterated through at least 10 rows using toLocalIterator.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUsing toLocalIterator():\")\n",
    "\n",
    "# toLocalIterator() trả về một iterator\n",
    "local_iterator = collectDF.toLocalIterator()\n",
    "\n",
    "# Chúng ta duyệt qua từng hàng trong iterator\n",
    "# Mỗi lần duyệt, Spark sẽ tải một partition dữ liệu về Driver (nếu chưa có)\n",
    "# và trả về các hàng từ partition đó.\n",
    "# Sau khi các hàng của một partition được xử lý, bộ nhớ có thể được giải phóng\n",
    "# trước khi tải partition tiếp theo.\n",
    "row_count = 0\n",
    "for row in local_iterator:\n",
    "    print(f\"  Processing row: {row}\")\n",
    "    row_count += 1\n",
    "\n",
    "print(f\"\\nSuccessfully iterated through at least {row_count} rows using toLocalIterator.\")\n",
    "\n",
    "# Bạn không thể duyệt lại iterator một lần nữa nếu không gọi lại toLocalIterator()\n",
    "# try:\n",
    "#     next(local_iterator)\n",
    "# except StopIteration:\n",
    "#     print(\"Iterator đã duyệt hết.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "782000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_iterator = collectDF.toLocalIterator()\n",
    "try:\n",
    "    next(local_iterator)\n",
    "except StopIteration:\n",
    "    print(\"Iterator đã duyệt hết.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4f7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
